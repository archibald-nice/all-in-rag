"""
================================================================================
固定字符数文本分割器 - CharacterTextSplitter 示例
================================================================================
功能说明：展示如何使用 CharacterTextSplitter 按固定字符数分割长文档。
          这是最基础的文本分割方法，按照指定的字符数进行切分。

适用场景：结构简单的文本、需要严格控制块大小的场景

技术栈：LangChain CharacterTextSplitter

注意：这种分割方式不考虑语义边界，可能会在句子中间截断
================================================================================
"""

# ============================================================================
# 导入必要的库
# ============================================================================

from langchain.text_splitter import CharacterTextSplitter
# CharacterTextSplitter: 固定字符数分割器
# 功能：按照指定的字符数和重叠量分割文本
# 参数说明：
#   - separator: 分隔符，默认为 "\n\n"（双换行）
#       用于优先在分隔符处切分，如果没有分隔符则按字符数切分
#   - chunk_size: 每个块的最大字符数（默认 4000）
#   - chunk_overlap: 块之间的重叠字符数（默认 200）
#       重叠有助于保持上下文连贯性，避免信息在边界处丢失
#   - length_function: 计算文本长度的函数（默认 len）
#       可以自定义来计算 token 数量或其他指标
#   - keep_separator: 是否保留分隔符（默认 False）
#
# 工作原理：
#   1. 首先尝试按 separator 分割文本
#   2. 如果分割后的块超过 chunk_size，则继续按字符数切分
#   3. 每个块包含 chunk_overlap 个字符的重叠部分

from langchain_community.document_loaders import TextLoader
# TextLoader: 纯文本文件加载器
# 参数说明：
#   - file_path: 文件路径
#   - encoding: 文件编码（常用 "utf-8", "gbk" 等）


# ============================================================================
# 第一步：文档加载
# ============================================================================

# 创建文本加载器
# TextLoader 用于加载纯文本文件
loader = TextLoader("../../data/C2/txt/蜂医.txt", encoding="utf-8")

# 执行加载
# 返回值：List[Document]，每个 Document 包含：
#   - page_content: 文档的文本内容
#   - metadata: 元数据字典（如来源文件路径）
docs = loader.load()


# ============================================================================
# 第二步：初始化分割器
# ============================================================================

# 创建固定大小分块器
# CharacterTextSplitter 会按照以下规则分割文本：
#   1. 首先尝试在换行符处分割
#   2. 如果某个段落在 chunk_size 内，则保持完整
#   3. 如果段落超过 chunk_size，则按字符数强行切分
#   4. 相邻块之间有 chunk_overlap 个字符的重叠
text_splitter = CharacterTextSplitter(
    chunk_size=200,     # 每个块最多 200 个字符
    chunk_overlap=10    # 相邻块之间重叠 10 个字符
)

# 示例说明：
# 假设原文是：这是一个很长的文本段落，包含了很多内容...
# 如果 chunk_size=200, chunk_overlap=10，则分割结果可能是：
#   块1: 这是一个很长的文本段落，包含了很多内容...（200字符）
#   块2: 的内容，继续还有更多文字...（200字符，前10字符与块1重叠）
#   块3: 更多文字，接着是后面的内容...（200字符，前10字符与块2重叠）


# ============================================================================
# 第三步：执行分块
# ============================================================================

# 执行文本分割
# split_documents() 方法会：
#   1. 遍历所有文档
#   2. 对每个文档的 page_content 进行分割
#   3. 保留原始文档的 metadata
#   4. 返回分割后的 Document 列表
chunks = text_splitter.split_documents(docs)


# ============================================================================
# 第四步：结果展示
# ============================================================================

# 打印分割结果统计
print(f"文本被切分为 {len(chunks)} 个块。\n")

# 展示前 5 个块的内容
print("--- 前5个块内容示例 ---")
for i, chunk in enumerate(chunks[:5]):
    print("=" * 60)
    # chunk 是一个 Document 对象
    # .page_content 属性包含实际的文本内容
    # .metadata 属性包含元数据信息
    print(f'块 {i+1} (长度: {len(chunk.page_content)}): "{chunk.page_content}"')


# ============================================================================
# 伪流程图：固定字符数分割流程
# ============================================================================
"""
┌─────────────────────────────────────────────────────────────────────┐
│                    固定字符数分割流程                                 │
└─────────────────────────────────────────────────────────────────────┘

  输入: 长文本文档
  ┌──────────────────────────────────────────────────────────────┐
  │                                                              │
  │  [1. 文档加载]                                                │
  │      ↓                                                       │
  │  TextLoader → docs (Document 对象)                           │
  │      ↓                                                       │
  │                                                              │
  │  [2. 初始化分割器]                                            │
  │      ↓                                                       │
  │  CharacterTextSplitter(                                      │
  │      chunk_size=200,                                         │
  │      chunk_overlap=10                                        │
  │  )                                                           │
  │      ↓                                                       │
  │                                                              │
  │  [3. 执行分割]                                                │
  │      ↓                                                       │
  │  ┌─────────────────────────────────────────────────────┐    │
  │  │                                                     │    │
  │  │  原文: 这是一个很长的段落，包含了很多内容，需要被分割  │    │
  │  │       成多个小块，每个块有固定的大小...             │    │
  │  │                                                     │    │
  │  │  ↓ 按字符数分割                                     │    │
  │  │                                                     │    │
  │  │  块1: 这是一个很长的段落，包含了很多内容，需要被 (200字) │    │
  │  │                                                     │    │
  │  │  块2: 容，需要被分 成多个小块，每个块有固定的大小 (200字) │    │
  │  │       ↑前10字与块1重叠                               │    │
  │  │                                                     │    │
  │  │  块3: 定的大小... (继续)                             │    │
  │  │                                                     │    │
  │  └─────────────────────────────────────────────────────┘    │
  │      ↓                                                       │
  │  chunks (List[Document])                                     │
  │                                                              │
  │  [4. 输出结果]                                               │
  │      ↓                                                       │
  │  - 统计块数量                                                 │
  │  - 展示每个块的内容和长度                                     │
  │      ↓                                                       │
  │  输出: 分割后的文本块列表                                      │
  │                                                              │
  └──────────────────────────────────────────────────────────────┘

  分割示例（chunk_size=200, chunk_overlap=10）:
  ┌──────────────────────────────────────────────────────────────┐
  │                                                              │
  │  原文: [................................................]     │
  │        (共500字符)                                            │
  │                                                              │
  │  分割后:                                                     │
  │  块1: [....................]  (字符 1-200)                     │
  │  块2:         [....................]  (字符 191-390)          │
  │              ↑ 重叠部分（字符 191-200）                        │
  │  块3:                  [....................]  (字符 381-500) │
  │                              ↑ 重叠部分（字符 381-390）        │
  │                                                              │
  └──────────────────────────────────────────────────────────────┘
"""


# ============================================================================
# 进阶用法说明
# ============================================================================
"""
# 1. 自定义分隔符
text_splitter = CharacterTextSplitter(
    separator="。",        # 使用中文句号作为分隔符
    chunk_size=200,
    chunk_overlap=10,
    keep_separator=True    # 保留分隔符
)

# 2. 使用 token 数而非字符数
import tiktoken

def token_count(text: str) -> int:
    encoder = tiktoken.get_encoding("cl100k_base")
    return len(encoder.encode(text))

text_splitter = CharacterTextSplitter(
    chunk_size=500,              # 500 个 token
    chunk_overlap=50,
    length_function=token_count   # 使用 token 计数
)

# 3. 处理中文文本
# 对于中文文本，可以不使用分隔符，直接按字符数分割
text_splitter = CharacterTextSplitter(
    separator="",           # 空字符串表示不使用分隔符
    chunk_size=200,
    chunk_overlap=10
)

# 4. 保留元数据
chunks = text_splitter.split_documents(docs)
for chunk in chunks:
    print(f"内容: {chunk.page_content[:50]}...")
    print(f"来源: {chunk.metadata.get('source', 'unknown')}")

# 5. 与其他分割器对比
# CharacterTextSplitter: 固定字符数，不考虑语义
# RecursiveCharacterTextSplitter: 递归尝试多种分隔符，保持语义
# SemanticChunker: 基于语义相似度分割，保持内容连贯性
"""
